{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    " #### Wasserstein distance\n",
    " 分布を荷物量とみなし、荷物を他の分布に移し替えるときにかかる最小コストを距離とする。  \n",
    " コンピュータサイエンスの領域では**Earth Mover's distance**と呼ばれる。  \n",
    "\n",
    " ## Code for NeurIPS 2019 paper \"Hierarchical Optimal Transport for Document Representation\"\n",
    " \n",
    " #### どんな論文？\n",
    " \n",
    " 文書間の類似距離測度における新しい尺度を提案。  \n",
    " Hierarchical Optimal Transport：Word Mover’s Distanceをさらに発展させた手法  \n",
    " \n",
    " \n",
    " #### Word Mover's Distanceとは？\n",
    " \n",
    " 文書間の距離をEarth Mover's Distanceをもって尺度とする手法  \n",
    " \n",
    " #### Earth Mover's Ditanceとは？\n",
    "   \n",
    " 対象の分布を荷物量とみなし、荷物を他の分布に移し替えるときに掛る最小コストを距離と考えるアプローチ。  \n",
    " Wasserstein distanceとも呼ばれる。\n",
    " \n",
    " 以下の図において、\n",
    " \n",
    " \n",
    " Pの各場所 $P_1,...,P_m$ には、重みの量だけ荷物が積まれているとします。  \n",
    " そして、Qの各場所　$Q_1,...,Q_n$ には重みの量だけ格納できる倉庫があるとします。  \n",
    " \n",
    "hierarchical optimal topic transport \n",
    " \n",
    " このとき、Pにある荷物全て・またはQに入る上限まで運ぶとすると、  \n",
    " どこからどこへどのくらい運ぶと最も効率が良いかを考えます。   \n",
    " \n",
    " \n",
    " \n",
    " ここで、$Pi$から$Qj$へ輸送する距離を $C_{ij}$ とし、$P_i$から$Q_j$へ輸送する荷物量を$Γ_{ij}$と定義します。  \n",
    " そして$P_i$から$Q_j$へ運ぶのに要する仕事量を$C_{ij}・Γ_{ij}$と定義します。 \n",
    " \n",
    " ここで、$P_i$から$Q_j$から輸送されるコストは下のように定義されます。  \n",
    " \n",
    " $$ W = \\sum^{m}_{i=1}\\sum^n_{j=1}C_{ij}Γ_{ij}$$\n",
    " \n",
    " EMDではこの最小値を距離とします。  \n",
    " 距離$C_{ij}$は固定されているので、最適化する変数は輸送量$Γ_{ij}$だけです。  \n",
    " \n",
    " \n",
    " 解き方は省略しますが、Hungarian algorithm(Kuhn,1955)を用いて解くと最適な$Γ^*_{ij}$が求まります。  \n",
    " EMDはこの$Γ^*_{ij}$を適用した場合の仕事量$W$（を輸送量について正規化したもの）です。\n",
    " \n",
    " EMDは輸送に必要な最小の仕事量が小さいほど二つの距離は近いという考え方になります。\n",
    " \n",
    " \n",
    " #### Word Mover's Distanceとは？\n",
    " \n",
    " EMDを文書間の類似度の計測に適用する手法。  \n",
    " 文書間の類似度を測る方法は、代表的なものとしてBag-of-Words表現のコサイン値があるが、  \n",
    " これは文書同士の共通語が少ない場合、BoWでは文書間の意味的な類似度を測ることが困難です。\n",
    " \n",
    " \n",
    " 上記の通り文書D0,D１には共通語こそありませんが、人間の目にはmediaとpressのようなそれぞれの語動詞は非常に近い意味を持つように見えますword2vecを用いればこのような語の意味理維持度をとらえた分散表現ベクトルが得られます。  \n",
    "\n",
    "\n",
    " $$X_w = (x_{x1},...,x_{wn})$$\n",
    " \n",
    " \n",
    " WMDではこの性質を利用して文書間の距離を求めます。  \n",
    " その考え方は大雑把に言えば、文書Aの語を類似する（＝分散表現間の距離が小さい）語で置き換えて文書Bに変換できるならば文書A,Bの類似度は大きい（＝距離は小さい）というようなものです。  \n",
    " \n",
    " \n",
    " 単語間の対応関係などわかりようないですし、そもそも文章間で単語の数から異なるでしょう。ですので、単語を一対一で対応づけるのではなく、下図のような重み付きの対応関係を考えます。  \n",
    " 重みには、図中の青線のように単語に紐つく重みの和が頻度に等しくなるよう制約をかけ、重みと単語間距離の積の総和が最小となるように重みを調整します。調整済みの重みと単語間距離の積の総和が2文間の距離（＝Word Mover’s Distance）となります。\n",
    " https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part1.html  \n",
    " \n",
    " \n",
    "# \n",
    "# \n",
    "# \n",
    " #### Word Mover's Distanceを扱う利点・不利な点\n",
    " \n",
    " - 利点\n",
    " 精度が高い  \n",
    " 解釈可能性が高い  \n",
    " \n",
    " \n",
    " - 不利な点  \n",
    " 計算量が大きい  \n",
    " 計算に時間がかかる\n",
    " \n",
    " \n",
    " #### Hierarchical Optimal Transportではこんな改善がありました\n",
    " WMDより計算時間が短い  \n",
    " 大きいサイズのコーパスにも適用可能  \n",
    " （WMDと同様）解釈可能性が高い  \n",
    " \n",
    "# \n",
    " https://ercim-news.ercim.eu/en112/r-i/faster-text-similarity-using-a-linear-complexity-relaxed-word-mover-s-distance\n",
    "# \n",
    " 最適化問題を解く際の制約を緩くすることで、文書中のすべての単語に対応しなくてもよい手法も提案している  \n",
    " (Relaxed Word Mover's Distance)\n",
    " \n",
    "# \n",
    " #### Hierarchical Optimal Transport\n",
    "# \n",
    " LDAで文書からトピックを抽出し、そのトピックにWMDを適用する手法\n",
    "# \n",
    "# \n",
    " あるトピックモデルがコーパス固有のトピック $T=(t_1, t_2, ..., t_{|T|}) \\in Δ^{|V|}$ を生成していると仮定します。  \n",
    " ここで、これは単語の分布であり、また文書の分布は次のように表せる。$\\bar{d^i} \\in Δ^{|T|}$  \n",
    " 今回WMDではトピックT間の離散的な輸送問題を考える。  \n",
    " ２つの文書$d^1, d^2$間の hierarchical optimal transoport distance(HOTT) を以下のように定義する。\n",
    " \n",
    " $$HOTT(d^1,d^2) = W_1(\\sum^{|T|}_{k=1}\\bar{d^1_k}\\delta_{tk}, \\sum^{|T|}_{k=1}\\bar{d^2_k}\\delta_{tk})$$\n",
    "\n",
    "\n",
    "\n",
    "ここで、|T|は全文書間のトピック数、$\\bar{d^1_k}, \\bar{d^2_k}$は対象の文書のトピックの分布  \n",
    "\n",
    "$\\delta_{t_k}$ はそれぞれトピック$t_k$に対応するディラックデルタ関数です\n",
    " \n",
    " \n",
    " \n",
    "それぞれトピック$t_k$に対応する確率分布\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " where each Dirac delta $\\delta_{tk}$ is a probability distribution only supported on the corresponding topic $t_k$ and where the ground metric is WMD between topics as distributions over words.   \n",
    "#   \n",
    " The resulting transport problem leverages topic correspondances provided by WMD in the base metric.\n",
    "# \n",
    " Out construction \n",
    "# \n",
    "# \n",
    " #### 実験・評価\n",
    " \n",
    " 実験内容：\n",
    " 処理時間  \n",
    " k-nn近傍法  \n",
    " t-SNE Visualization  \n",
    " Sentivity to Parameters\n",
    "# \n",
    "# \n",
    " Baseline手法：  \n",
    " nBOW  \n",
    " LSI  \n",
    " SIF  \n",
    " LDA  \n",
    " Cosine  \n",
    " RWMD  \n",
    " TF-IDF  \n",
    " HOFTT  \n",
    " HOTT  \n",
    " WMD-T20  \n",
    "# \n",
    " 使用したデータ：  \n",
    " ohsumed  \n",
    " 20news  \n",
    " twitter  \n",
    " gutenberg  \n",
    " amazon  \n",
    " r8  \n",
    " bbcsport  \n",
    " classic  \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    " #### Summary\n",
    "# \n",
    " この論文ではWMDの代替としてトピックモデルの階層的な潜在構造と、単語の埋め込みからのジオメトリを組み合わせます。\n",
    " 単語の埋め込みからの言語情報と、潜在ディレクれ割り当てからのコーパス固有の意味的に意味のあるトピック分布を組み合わせた階層的な最適トピック転送（）HOTTドキュメントを提案します。\n",
    " このドキュメント距離はWMDよりも効率的で解釈しやすい。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
